{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŒ™ EALLIS: Enhanced Adaptive Low-Light Instance Segmentation â€” Training Notebook\n",
                "\n",
                "This notebook trains the **Mask R-CNN + AWD + SCB + DSL** model.\n",
                "\n",
                "**Training**: COCO train2017 with synthetic noise pipeline (SynCOCO)  \n",
                "**Evaluation**: EALLIS test set (real-world dark images)  \n",
                "**Model**: `MaskRCNNNoiseInv` with `ResNetAdaDSmoothPrior` backbone\n",
                "\n",
                "---\n",
                "âš ï¸ **Requirements**: GPU runtime (T4 or better). Go to `Runtime > Change runtime type > GPU`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Check GPU & Environment"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!nvidia-smi\n",
                "import torch\n",
                "print(f'PyTorch: {torch.__version__}')\n",
                "print(f'CUDA available: {torch.cuda.is_available()}')\n",
                "if torch.cuda.is_available():\n",
                "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Install Dependencies"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "torch_version = torch.__version__.split('+')[0]\n",
                "cuda_version = torch.version.cuda.replace('.', '')\n",
                "print(f'PyTorch: {torch_version}, CUDA: {cuda_version}')\n",
                "\n",
                "!pip install mmcv-full==1.7.2 -f https://download.openmmlab.com/mmcv/dist/cu{cuda_version}/torch{torch_version}/index.html\n",
                "!pip install pycocotools scikit-learn terminaltables numpy==1.26.4"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Clone the EALLIS Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "REPO_DIR = '/content/EALLIS'\n",
                "\n",
                "if not os.path.exists(REPO_DIR):\n",
                "    !git clone https://github.com/itzaqeel/EALLIS.git {REPO_DIR}\n",
                "else:\n",
                "    print(f'{REPO_DIR} already exists, skipping clone.')\n",
                "\n",
                "os.chdir(REPO_DIR)\n",
                "print(f'Working directory: {os.getcwd()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Install mmdetection (from local repo)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(os.path.join(REPO_DIR, 'mmdetection'))\n",
                "!pip install -e . --no-deps\n",
                "os.chdir(REPO_DIR)\n",
                "\n",
                "import mmdet\n",
                "print(f'mmdet version: {mmdet.__version__}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Apply All Fixes\n",
                "\n",
                "Fixes relative imports, registration conflicts, and deprecated APIs in `mmdetection_custom_part`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob\n",
                "import re\n",
                "\n",
                "os.chdir(REPO_DIR)\n",
                "\n",
                "# ---------- Fix 1: Bump mmcv version cap ----------\n",
                "init_file = 'mmdetection/mmdet/__init__.py'\n",
                "with open(init_file, 'r') as f:\n",
                "    content = f.read()\n",
                "content = re.sub(r\"mmcv_maximum_version\\s*=\\s*'[^']*'\", \"mmcv_maximum_version = '2.1.0'\", content)\n",
                "with open(init_file, 'w') as f:\n",
                "    f.write(content)\n",
                "print('[Fix 1] mmcv version cap updated.')\n",
                "\n",
                "# ---------- Fix 2: Comment out deprecated imports ----------\n",
                "for resnet_file in [\n",
                "    'mmdetection/mmdet/models/backbones/resnet.py',\n",
                "    'mmdetection_custom_part/mmdet/models/backbones/resnet.py'\n",
                "]:\n",
                "    if os.path.exists(resnet_file):\n",
                "        with open(resnet_file, 'r') as f:\n",
                "            content = f.read()\n",
                "        content = content.replace('from os import pread', '# from os import pread')\n",
                "        content = content.replace('from torch.functional import _index_tensor_with_indices_list',\n",
                "                                  '# from torch.functional import _index_tensor_with_indices_list')\n",
                "        with open(resnet_file, 'w') as f:\n",
                "            f.write(content)\n",
                "print('[Fix 2] Deprecated imports commented out.')\n",
                "\n",
                "# ---------- Fix 3: Fix relative imports in custom_part ----------\n",
                "import_fixes = [\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/resnet.py',\n",
                "     'from ..utils', 'from mmdet.models.utils'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/resnet.py',\n",
                "     'from .cbam', 'from mmdet.models.backbones.cbam'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/resnext.py',\n",
                "     'from ..utils', 'from mmdet.models.utils'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/swin.py',\n",
                "     'from ...utils', 'from mmdet.utils'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/swin.py',\n",
                "     'from ..utils.ckpt_convert', 'from mmdet.models.utils.ckpt_convert'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/swin.py',\n",
                "     'from ..utils.transformer', 'from mmdet.models.utils.transformer'),\n",
                "    ('mmdetection_custom_part/mmdet/models/dense_heads/maskformer_head.py',\n",
                "     'from .anchor_free_head', 'from mmdet.models.dense_heads.anchor_free_head'),\n",
                "    ('mmdetection_custom_part/mmdet/models/dense_heads/mask2former_head.py',\n",
                "     'from .anchor_free_head', 'from mmdet.models.dense_heads.anchor_free_head'),\n",
                "    ('mmdetection_custom_part/mmdet/models/detectors/two_stage.py',\n",
                "     'from .base', 'from mmdet.models.detectors.base'),\n",
                "    ('mmdetection_custom_part/mmdet/models/detectors/maskformer.py',\n",
                "     'from .single_stage', 'from mmdet.models.detectors.single_stage'),\n",
                "    ('mmdetection_custom_part/mmdet/models/seg_heads/base_semantic_head.py',\n",
                "     'from ..utils import interpolate_as', 'from mmdet.models.utils import interpolate_as'),\n",
                "    ('mmdetection_custom_part/mmdet/models/seg_heads/panoptic_fpn_head.py',\n",
                "     'from ..utils import ConvUpsample', 'from mmdet.models.utils import ConvUpsample'),\n",
                "]\n",
                "for filepath, old_imp, new_imp in import_fixes:\n",
                "    if os.path.exists(filepath):\n",
                "        with open(filepath, 'r') as f:\n",
                "            content = f.read()\n",
                "        if old_imp in content:\n",
                "            content = content.replace(old_imp, new_imp)\n",
                "            with open(filepath, 'w') as f:\n",
                "                f.write(content)\n",
                "print('[Fix 3] Relative imports fixed.')\n",
                "\n",
                "# ---------- Fix 4: Fix auxiliary module imports in detectors ----------\n",
                "for det_file in [\n",
                "    'mmdetection_custom_part/mmdet/models/detectors/mask_rcnn.py',\n",
                "    'mmdetection_custom_part/mmdet/models/detectors/faster_rcnn_noise_inv.py'\n",
                "]:\n",
                "    if os.path.exists(det_file):\n",
                "        with open(det_file, 'r') as f:\n",
                "            content = f.read()\n",
                "        content = content.replace('from ..backbones.aux_modules', 'from mmdet.models.backbones.aux_modules')\n",
                "        content = content.replace('from ..backbones.multiscale_discriminator', 'from mmdet.models.backbones.multiscale_discriminator')\n",
                "        content = content.replace('from ..backbones.lsid', 'from mmdet.models.backbones.lsid')\n",
                "        with open(det_file, 'w') as f:\n",
                "            f.write(content)\n",
                "print('[Fix 4] Detector auxiliary imports fixed.')\n",
                "\n",
                "# ---------- Fix 5: Force ALL register_module() calls ----------\n",
                "custom_dir = 'mmdetection_custom_part'\n",
                "count = 0\n",
                "for py_file in glob.glob(os.path.join(custom_dir, '**', '*.py'), recursive=True):\n",
                "    with open(py_file, 'r') as f:\n",
                "        content = f.read()\n",
                "    if '.register_module()' in content:\n",
                "        new_content = content.replace('.register_module()', '.register_module(force=True)')\n",
                "        with open(py_file, 'w') as f:\n",
                "            f.write(new_content)\n",
                "        count += 1\n",
                "print(f'[Fix 5] Forced registration in {count} files.')\n",
                "\n",
                "# ---------- Fix 6: Rewrite __init__.py files ----------\n",
                "init_content = '''from .resnet import ResNet, ResNetV1d, ResNetAdaD, ResNetAdaDSmoothPrior\n",
                "from .resnext import ResNeXt\n",
                "from .swin import SwinTransformer, SwinTransformerV2\n",
                "from .convnext import ConvNeXt\n",
                "\n",
                "__all__ = ['ResNet', 'ResNetV1d', 'ResNeXt', 'SwinTransformer', 'SwinTransformerV2',\n",
                "           'ConvNeXt', 'ResNetAdaD', 'ResNetAdaDSmoothPrior']\n",
                "'''\n",
                "with open('mmdetection_custom_part/mmdet/models/backbones/__init__.py', 'w') as f:\n",
                "    f.write(init_content)\n",
                "\n",
                "init_content = '''from .maskformer_head import MaskFormerHead\n",
                "from .mask2former_head import Mask2FormerHead\n",
                "\n",
                "__all__ = ['MaskFormerHead', 'Mask2FormerHead']\n",
                "'''\n",
                "with open('mmdetection_custom_part/mmdet/models/dense_heads/__init__.py', 'w') as f:\n",
                "    f.write(init_content)\n",
                "\n",
                "init_content = '''from .two_stage import TwoStageDetector\n",
                "from .faster_rcnn import FasterRCNN\n",
                "from .faster_rcnn_noise_inv import FasterRCNNNoiseInv\n",
                "from .mask_rcnn import MaskRCNN, MaskRCNNNoiseInv as MaskRCNNNoiseInvDet\n",
                "from .maskformer import MaskFormer\n",
                "from .mask2former import Mask2Former\n",
                "\n",
                "__all__ = ['TwoStageDetector', 'FasterRCNN', 'FasterRCNNNoiseInv',\n",
                "           'MaskRCNN', 'MaskRCNNNoiseInvDet', 'MaskFormer', 'Mask2Former']\n",
                "'''\n",
                "with open('mmdetection_custom_part/mmdet/models/detectors/__init__.py', 'w') as f:\n",
                "    f.write(init_content)\n",
                "\n",
                "models_init = 'mmdetection_custom_part/mmdet/models/__init__.py'\n",
                "if os.path.exists(models_init):\n",
                "    with open(models_init, 'r') as f:\n",
                "        content = f.read()\n",
                "    content = content.replace('from .necks import *', '# from .necks import *')\n",
                "    with open(models_init, 'w') as f:\n",
                "        f.write(content)\n",
                "\n",
                "print('[Fix 6] __init__.py files rewritten.')\n",
                "print('\\nâœ… All fixes applied successfully!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Verify Custom Modules Import"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, REPO_DIR)\n",
                "sys.path.insert(0, os.path.join(REPO_DIR, 'mmdetection'))\n",
                "\n",
                "try:\n",
                "    import mmdetection_custom_part.mmdet.models.detectors\n",
                "    import mmdetection_custom_part.mmdet.models.backbones\n",
                "    import mmdetection_custom_part.mmdet.models.dense_heads\n",
                "    import mmdetection_custom_part.mmdet.models.roi_heads\n",
                "    import mmdetection_custom_part.mmdet.models.plugins\n",
                "    import mmdetection_custom_part.mmdet.models.seg_heads\n",
                "    import mmdetection_custom_part.mmdet.models.losses\n",
                "    print('âœ… All custom modules imported successfully!')\n",
                "except Exception as e:\n",
                "    import traceback\n",
                "    traceback.print_exc()\n",
                "    raise RuntimeError(f'Custom module import failed: {e}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Setup Datasets\n",
                "\n",
                "### Training Data (COCO â†’ SynCOCO)\n",
                "Clean COCO images go through the `AddNoisyImg` pipeline during training to create synthetic low-light data.\n",
                "\n",
                "### Evaluation Data (EALLIS)\n",
                "Upload your EALLIS dataset from Google Drive."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(REPO_DIR)\n",
                "\n",
                "# Create data directories\n",
                "os.makedirs('data/coco/annotations', exist_ok=True)\n",
                "os.makedirs('data/coco/train2017', exist_ok=True)\n",
                "os.makedirs('data/coco/val2017', exist_ok=True)\n",
                "os.makedirs('data/eallis/annotations', exist_ok=True)\n",
                "os.makedirs('data/eallis/images', exist_ok=True)\n",
                "\n",
                "print('Data directories created.')\n",
                "print('\\nExpected structure:')\n",
                "print('data/')\n",
                "print('â”œâ”€â”€ coco/              (training - clean images, noise added on-the-fly)')\n",
                "print('â”‚   â”œâ”€â”€ annotations/   (instances_train2017.json)')\n",
                "print('â”‚   â””â”€â”€ train2017/     (118K COCO training images)')\n",
                "print('â””â”€â”€ eallis/            (evaluation - real dark images)')\n",
                "print('    â”œâ”€â”€ annotations/   (eallis_coco_JPG_test+1.json)')\n",
                "print('    â””â”€â”€ images/        (EALLIS test images)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7a. Download COCO Dataset\n",
                "\n",
                "âš ï¸ This downloads ~20GB. If you already have COCO on Google Drive, use Option B below instead."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option A: Download COCO directly (~15-20 min)\n",
                "DOWNLOAD_COCO = True  # Set to False if using Option B\n",
                "\n",
                "if DOWNLOAD_COCO:\n",
                "    os.chdir(os.path.join(REPO_DIR, 'data/coco'))\n",
                "\n",
                "    # Annotations\n",
                "    !wget -q http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
                "    !unzip -qo annotations_trainval2017.zip\n",
                "    !rm annotations_trainval2017.zip\n",
                "\n",
                "    # Train images\n",
                "    !wget -q http://images.cocodataset.org/zips/train2017.zip\n",
                "    !unzip -qo train2017.zip\n",
                "    !rm train2017.zip\n",
                "\n",
                "    os.chdir(REPO_DIR)\n",
                "    print(f'COCO train images: {len(os.listdir(\"data/coco/train2017\"))}')\n",
                "else:\n",
                "    print('Skipping COCO download. Using Option B (Drive symlinks).')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Option B: Symlink COCO from Google Drive (if already uploaded)\n",
                "# Uncomment and adjust paths:\n",
                "\n",
                "# from google.colab import drive\n",
                "# drive.mount('/content/drive')\n",
                "# !ln -sf /content/drive/MyDrive/datasets/coco/train2017 data/coco/train2017\n",
                "# !ln -sf /content/drive/MyDrive/datasets/coco/annotations data/coco/annotations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7b. Upload EALLIS Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "# Adjust this path to your EALLIS dataset location on Drive\n",
                "EALLIS_DRIVE_PATH = '/content/drive/MyDrive/datasets/eallis'\n",
                "\n",
                "if os.path.exists(EALLIS_DRIVE_PATH):\n",
                "    if os.path.exists(os.path.join(EALLIS_DRIVE_PATH, 'annotations')):\n",
                "        !cp -r {EALLIS_DRIVE_PATH}/annotations/* {REPO_DIR}/data/eallis/annotations/\n",
                "    if os.path.exists(os.path.join(EALLIS_DRIVE_PATH, 'images')):\n",
                "        !cp -r {EALLIS_DRIVE_PATH}/images/* {REPO_DIR}/data/eallis/images/\n",
                "    elif os.path.exists(os.path.join(EALLIS_DRIVE_PATH, 'JPEGImages')):\n",
                "        !cp -r {EALLIS_DRIVE_PATH}/JPEGImages/* {REPO_DIR}/data/eallis/images/\n",
                "    print(f'EALLIS annotations: {os.listdir(REPO_DIR + \"/data/eallis/annotations\")}')\n",
                "    print(f'EALLIS images: {len(os.listdir(REPO_DIR + \"/data/eallis/images\"))} files')\n",
                "else:\n",
                "    print(f'EALLIS not found at {EALLIS_DRIVE_PATH}. Update the path above.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create symlink for JPEGImages (annotations reference ./JPEGImages/xxx.JPG)\n",
                "os.chdir(REPO_DIR)\n",
                "jpeg_link = 'data/eallis/JPEGImages'\n",
                "if not os.path.exists(jpeg_link):\n",
                "    os.symlink(os.path.abspath('data/eallis/images'), jpeg_link)\n",
                "    print(f'Created symlink: {jpeg_link} -> data/eallis/images')\n",
                "print(f'EALLIS images accessible: {os.path.exists(\"data/eallis/JPEGImages\")}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Prepare Training Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(REPO_DIR)\n",
                "\n",
                "CONFIG_FILE = 'Configs/mask_rcnn_r50_fpn_caffe_AWD_SCB_DSL_SynCOCO2EALLIS.py'\n",
                "\n",
                "with open(CONFIG_FILE, 'r') as f:\n",
                "    config_content = f.read()\n",
                "\n",
                "# Reduce batch size for Colab GPU (T4 = 16GB)\n",
                "config_content = config_content.replace('BATCHSIZE = 8', 'BATCHSIZE = 4')\n",
                "\n",
                "# Train from scratch with pretrained backbone\n",
                "config_content = re.sub(\n",
                "    r\"load_from\\s*=\\s*'[^']*'\",\n",
                "    \"load_from = None\",\n",
                "    config_content)\n",
                "\n",
                "TRAIN_CONFIG = 'Configs/train_colab.py'\n",
                "with open(TRAIN_CONFIG, 'w') as f:\n",
                "    f.write(config_content)\n",
                "\n",
                "print(f'Training config saved to: {TRAIN_CONFIG}')\n",
                "print('  - Train: COCO train2017 â†’ SynCOCO (noise added on-the-fly via AddNoisyImg)')\n",
                "print('  - Val/Test: EALLIS test set')\n",
                "print('  - Batch size: 4, Epochs: 12')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Start Training ðŸš€\n",
                "\n",
                "Training flow: Clean COCO image â†’ `AddNoisyImg` (synthetic noise) â†’ Model receives both `img` (clean) and `noisy_img` (dark) â†’ DSL loss"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(REPO_DIR)\n",
                "\n",
                "import sys\n",
                "sys.path.insert(0, REPO_DIR)\n",
                "sys.path.insert(0, os.path.join(REPO_DIR, 'mmdetection'))\n",
                "\n",
                "import mmdetection_custom_part.mmdet.models.detectors\n",
                "import mmdetection_custom_part.mmdet.models.backbones\n",
                "import mmdetection_custom_part.mmdet.models.dense_heads\n",
                "import mmdetection_custom_part.mmdet.models.roi_heads\n",
                "import mmdetection_custom_part.mmdet.models.plugins\n",
                "import mmdetection_custom_part.mmdet.models.seg_heads\n",
                "import mmdetection_custom_part.mmdet.models.losses\n",
                "\n",
                "import torch\n",
                "from mmcv import Config\n",
                "from mmdet.datasets import build_dataset\n",
                "from mmdet.models import build_detector\n",
                "from mmdet.apis import train_detector\n",
                "\n",
                "cfg = Config.fromfile('Configs/train_colab.py')\n",
                "\n",
                "print('Building datasets...')\n",
                "datasets = [build_dataset(cfg.data.train)]\n",
                "print(f'Train dataset: {len(datasets[0])} images (COCO â†’ SynCOCO via AddNoisyImg)')\n",
                "\n",
                "print('Building model...')\n",
                "model = build_detector(cfg.model)\n",
                "model.init_weights()\n",
                "model.CLASSES = datasets[0].CLASSES\n",
                "\n",
                "print(f'Model: {cfg.model.type}')\n",
                "print(f'Backbone: {cfg.model.backbone.type}')\n",
                "print(f'Starting training for {cfg.runner.max_epochs} epochs...')\n",
                "\n",
                "train_detector(model, datasets, cfg, distributed=False, validate=True, meta=dict())\n",
                "\n",
                "print('\\nâœ… Training complete!')\n",
                "print(f'Checkpoints saved to: {cfg.work_dir}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Evaluate on EALLIS Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(REPO_DIR)\n",
                "\n",
                "from mmcv.parallel import MMDataParallel\n",
                "from mmcv.runner import load_checkpoint\n",
                "from mmdet.datasets import build_dataloader\n",
                "from mmdet.apis import single_gpu_test\n",
                "\n",
                "import glob\n",
                "ckpt_files = sorted(glob.glob(os.path.join(cfg.work_dir, 'epoch_*.pth')))\n",
                "latest_ckpt = ckpt_files[-1] if ckpt_files else os.path.join(cfg.work_dir, 'latest.pth')\n",
                "print(f'Using checkpoint: {latest_ckpt}')\n",
                "\n",
                "test_dataset = build_dataset(cfg.data.test)\n",
                "test_loader = build_dataloader(test_dataset, samples_per_gpu=1, workers_per_gpu=2, dist=False, shuffle=False)\n",
                "\n",
                "cfg.model.pretrained = None\n",
                "cfg.model.backbone.init_cfg = None\n",
                "eval_model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
                "checkpoint = load_checkpoint(eval_model, latest_ckpt, map_location='cpu')\n",
                "eval_model.CLASSES = test_dataset.CLASSES\n",
                "eval_model = MMDataParallel(eval_model, device_ids=[0])\n",
                "eval_model.eval()\n",
                "\n",
                "print(f'Running inference on {len(test_dataset)} images...')\n",
                "results = single_gpu_test(eval_model, test_loader, show=False)\n",
                "\n",
                "eval_results = test_dataset.evaluate(results, metric=['bbox', 'segm'])\n",
                "print('\\n' + '='*60)\n",
                "print('EVALUATION RESULTS')\n",
                "print('='*60)\n",
                "for key, val in eval_results.items():\n",
                "    print(f'  {key}: {val:.4f}' if isinstance(val, float) else f'  {key}: {val}')\n",
                "print('='*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Save Checkpoints to Google Drive"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "\n",
                "SAVE_DIR = '/content/drive/MyDrive/EALLIS_checkpoints'\n",
                "os.makedirs(SAVE_DIR, exist_ok=True)\n",
                "\n",
                "for ckpt in glob.glob(os.path.join(cfg.work_dir, '*.pth')):\n",
                "    shutil.copy2(ckpt, os.path.join(SAVE_DIR, os.path.basename(ckpt)))\n",
                "    print(f'Saved: {os.path.basename(ckpt)}')\n",
                "\n",
                "for log_file in glob.glob(os.path.join(cfg.work_dir, '*.log*')):\n",
                "    shutil.copy2(log_file, os.path.join(SAVE_DIR, os.path.basename(log_file)))\n",
                "\n",
                "print(f'\\nâœ… All files saved to: {SAVE_DIR}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Tips\n",
                "\n",
                "- **OOM errors**: Reduce `BATCHSIZE` in cell 8 (try 2 or 1)\n",
                "- **Faster training**: Use A100 via `Runtime > Change runtime type`\n",
                "- **Resume training**: Set `resume_from = 'work_dir/epoch_X.pth'` in config\n",
                "- **Fine-tune from pretrained**: Set `load_from = 'Checkpoints/Checkpoint1.pth'`\n",
                "- **Colab timeout**: Save checkpoints to Google Drive regularly (cell 11)"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}