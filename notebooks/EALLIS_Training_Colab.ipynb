{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸŒ™ EALLIS: Enhanced Adaptive Low-Light Instance Segmentation â€” Training Notebook\n",
                "\n",
                "This notebook trains the **Mask R-CNN + AWD + SCB + DSL** model.\n",
                "\n",
                "**Training**: COCO train2017 with synthetic noise pipeline (SynCOCO)  \n",
                "**Evaluation**: EALLIS test set (real-world dark images)  \n",
                "**Model**: `MaskRCNNNoiseInv` with `ResNetAdaDSmoothPrior` backbone\n",
                "\n",
                "---\n",
                "âš ï¸ **Requirements**: GPU runtime (T4 or better). Go to `Runtime > Change runtime type > GPU`."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Mount Google Drive & Check GPU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')\n",
                "\n",
                "!nvidia-smi\n",
                "import torch, sys\n",
                "print(f'PyTorch: {torch.__version__}')\n",
                "print(f'CUDA: {torch.version.cuda}')\n",
                "print(f'Python: {sys.version}')\n",
                "print(f'GPU available: {torch.cuda.is_available()}')\n",
                "if torch.cuda.is_available():\n",
                "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Install Dependencies\n",
                "\n",
                "This cell caches the compiled mmcv-full on Google Drive. First run takes ~20-30 min, but subsequent sessions install in **~30 seconds** from the cached wheel."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, glob, time\n",
                "import torch\n",
                "\n",
                "torch_ver = torch.__version__.split('+')[0]\n",
                "cuda_ver = torch.version.cuda.replace('.', '')\n",
                "py_ver = f'{sys.version_info.major}{sys.version_info.minor}'\n",
                "\n",
                "# Cache directory on Google Drive\n",
                "CACHE_DIR = '/content/drive/MyDrive/EALLIS_pip_cache'\n",
                "os.makedirs(CACHE_DIR, exist_ok=True)\n",
                "\n",
                "# Check if mmcv is already installed\n",
                "try:\n",
                "    import mmcv\n",
                "    print(f'âœ… mmcv-full {mmcv.__version__} already installed! Skipping.')\n",
                "except ImportError:\n",
                "    # Check for cached wheel on Drive\n",
                "    cached_wheels = glob.glob(os.path.join(CACHE_DIR, 'mmcv_full-1.7.2*.whl'))\n",
                "    \n",
                "    if cached_wheels:\n",
                "        print(f'ðŸ“¦ Found cached wheel on Drive, installing (~30 sec)...')\n",
                "        wheel_path = cached_wheels[0]\n",
                "        os.system(f'pip install {wheel_path}')\n",
                "    else:\n",
                "        print(f'â³ No cached wheel found. Compiling mmcv-full from source (~20-30 min)...')\n",
                "        print(f'   PyTorch {torch_ver}, CUDA {cuda_ver}, Python {py_ver}')\n",
                "        print('='*60)\n",
                "        \n",
                "        # Build a wheel so we can cache it\n",
                "        os.system(f'pip wheel mmcv-full==1.7.2 -w /tmp/mmcv_wheels '\n",
                "                  f'-f https://download.openmmlab.com/mmcv/dist/cu{cuda_ver}/torch{torch_ver}/index.html '\n",
                "                  f'2>&1 | tail -5')\n",
                "        \n",
                "        # Install the built wheel\n",
                "        built_wheels = glob.glob('/tmp/mmcv_wheels/mmcv_full-1.7.2*.whl')\n",
                "        if built_wheels:\n",
                "            wheel_path = built_wheels[0]\n",
                "            os.system(f'pip install {wheel_path}')\n",
                "            # Cache to Drive for next time\n",
                "            import shutil\n",
                "            shutil.copy2(wheel_path, CACHE_DIR)\n",
                "            print(f'\\nðŸ’¾ Cached wheel to Drive for future sessions!')\n",
                "        else:\n",
                "            # Fallback: direct pip install\n",
                "            os.system(f'pip install mmcv-full==1.7.2 '\n",
                "                      f'-f https://download.openmmlab.com/mmcv/dist/cu{cuda_ver}/torch{torch_ver}/index.html')\n",
                "    \n",
                "    import mmcv\n",
                "    print(f'\\nâœ… mmcv-full {mmcv.__version__} installed!')\n",
                "\n",
                "# Other dependencies (fast)\n",
                "!pip install -q pycocotools scikit-learn terminaltables pretrainedmodels\n",
                "print('âœ… All dependencies ready!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Clone the EALLIS Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "\n",
                "REPO_DIR = '/content/EALLIS'\n",
                "\n",
                "if not os.path.exists(REPO_DIR):\n",
                "    !git clone https://github.com/itzaqeel/EALLIS.git {REPO_DIR}\n",
                "else:\n",
                "    print(f'{REPO_DIR} already exists, skipping clone.')\n",
                "\n",
                "os.chdir(REPO_DIR)\n",
                "print(f'Working directory: {os.getcwd()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Install mmdetection (from local repo)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "os.chdir(os.path.join(REPO_DIR, 'mmdetection'))\n",
                "!pip install -e . --no-deps\n",
                "os.chdir(REPO_DIR)\n",
                "\n",
                "sys.path.insert(0, os.path.join(REPO_DIR, 'mmdetection'))\n",
                "\n",
                "import mmdet\n",
                "print(f'mmdet version: {mmdet.__version__}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Apply All Fixes\n",
                "\n",
                "Fixes compatibility issues with Python 3.12, mmcv 1.7.2, and the custom modules."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob\n",
                "import re\n",
                "\n",
                "os.chdir(REPO_DIR)\n",
                "\n",
                "# Fix 1: Bump mmcv version cap\n",
                "init_file = 'mmdetection/mmdet/__init__.py'\n",
                "with open(init_file, 'r') as f:\n",
                "    content = f.read()\n",
                "content = re.sub(r\"mmcv_maximum_version\\s*=\\s*'[^']*'\", \"mmcv_maximum_version = '3.0.0'\", content)\n",
                "with open(init_file, 'w') as f:\n",
                "    f.write(content)\n",
                "print('[Fix 1] mmcv version cap updated.')\n",
                "\n",
                "# Fix 2: Comment out deprecated/removed imports (Python 3.12 compatibility)\n",
                "for resnet_file in [\n",
                "    'mmdetection/mmdet/models/backbones/resnet.py',\n",
                "    'mmdetection_custom_part/mmdet/models/backbones/resnet.py'\n",
                "]:\n",
                "    if os.path.exists(resnet_file):\n",
                "        with open(resnet_file, 'r') as f:\n",
                "            content = f.read()\n",
                "        content = content.replace('import imp', '# import imp  # removed in Python 3.12')\n",
                "        content = content.replace('from os import pread', '# from os import pread')\n",
                "        content = content.replace('from tokenize import group', '# from tokenize import group')\n",
                "        content = content.replace('from numpy.core.fromnumeric import size', '# from numpy.core.fromnumeric import size')\n",
                "        content = content.replace('from torch.functional import _index_tensor_with_indices_list',\n",
                "                                  '# from torch.functional import _index_tensor_with_indices_list')\n",
                "        with open(resnet_file, 'w') as f:\n",
                "            f.write(content)\n",
                "print('[Fix 2] Python 3.12 deprecated imports fixed.')\n",
                "\n",
                "# Fix 3: Fix relative imports\n",
                "import_fixes = [\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/resnet.py', 'from ..utils', 'from mmdet.models.utils'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/resnet.py', 'from .cbam', 'from mmdet.models.backbones.cbam'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/resnext.py', 'from ..utils', 'from mmdet.models.utils'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/swin.py', 'from ...utils', 'from mmdet.utils'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/swin.py', 'from ..utils.ckpt_convert', 'from mmdet.models.utils.ckpt_convert'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/swin.py', 'from ..utils.transformer', 'from mmdet.models.utils.transformer'),\n",
                "    ('mmdetection_custom_part/mmdet/models/dense_heads/maskformer_head.py', 'from .anchor_free_head', 'from mmdet.models.dense_heads.anchor_free_head'),\n",
                "    ('mmdetection_custom_part/mmdet/models/dense_heads/mask2former_head.py', 'from .anchor_free_head', 'from mmdet.models.dense_heads.anchor_free_head'),\n",
                "    ('mmdetection_custom_part/mmdet/models/detectors/two_stage.py', 'from .base', 'from mmdet.models.detectors.base'),\n",
                "    ('mmdetection_custom_part/mmdet/models/detectors/maskformer.py', 'from .single_stage', 'from mmdet.models.detectors.single_stage'),\n",
                "    ('mmdetection_custom_part/mmdet/models/seg_heads/base_semantic_head.py', 'from ..utils import interpolate_as', 'from mmdet.models.utils import interpolate_as'),\n",
                "    ('mmdetection_custom_part/mmdet/models/seg_heads/panoptic_fpn_head.py', 'from ..utils import ConvUpsample', 'from mmdet.models.utils import ConvUpsample'),\n",
                "]\n",
                "for filepath, old_imp, new_imp in import_fixes:\n",
                "    if os.path.exists(filepath):\n",
                "        with open(filepath, 'r') as f:\n",
                "            content = f.read()\n",
                "        if old_imp in content:\n",
                "            content = content.replace(old_imp, new_imp)\n",
                "            with open(filepath, 'w') as f:\n",
                "                f.write(content)\n",
                "print('[Fix 3] Relative imports fixed.')\n",
                "\n",
                "# Fix 4: Detector auxiliary imports\n",
                "for det_file in [\n",
                "    'mmdetection_custom_part/mmdet/models/detectors/mask_rcnn.py',\n",
                "    'mmdetection_custom_part/mmdet/models/detectors/faster_rcnn_noise_inv.py'\n",
                "]:\n",
                "    if os.path.exists(det_file):\n",
                "        with open(det_file, 'r') as f:\n",
                "            content = f.read()\n",
                "        content = content.replace('from ..backbones.aux_modules', 'from mmdet.models.backbones.aux_modules')\n",
                "        content = content.replace('from ..backbones.multiscale_discriminator', 'from mmdet.models.backbones.multiscale_discriminator')\n",
                "        content = content.replace('from ..backbones.lsid', 'from mmdet.models.backbones.lsid')\n",
                "        with open(det_file, 'w') as f:\n",
                "            f.write(content)\n",
                "print('[Fix 4] Detector auxiliary imports fixed.')\n",
                "\n",
                "# Fix 5: Force register_module()\n",
                "custom_dir = 'mmdetection_custom_part'\n",
                "count = 0\n",
                "for py_file in glob.glob(os.path.join(custom_dir, '**', '*.py'), recursive=True):\n",
                "    with open(py_file, 'r') as f:\n",
                "        content = f.read()\n",
                "    if '.register_module()' in content:\n",
                "        new_content = content.replace('.register_module()', '.register_module(force=True)')\n",
                "        with open(py_file, 'w') as f:\n",
                "            f.write(new_content)\n",
                "        count += 1\n",
                "print(f'[Fix 5] Forced registration in {count} files.')\n",
                "\n",
                "# Fix 6: Rewrite __init__.py files\n",
                "with open('mmdetection_custom_part/mmdet/models/backbones/__init__.py', 'w') as f:\n",
                "    f.write(\"\"\"from .resnet import ResNet, ResNetV1d, ResNetAdaD, ResNetAdaDSmoothPrior\n",
                "from .resnext import ResNeXt\n",
                "from .swin import SwinTransformer, SwinTransformerV2\n",
                "from .convnext import ConvNeXt\n",
                "__all__ = ['ResNet', 'ResNetV1d', 'ResNeXt', 'SwinTransformer', 'SwinTransformerV2',\n",
                "           'ConvNeXt', 'ResNetAdaD', 'ResNetAdaDSmoothPrior']\n",
                "\"\"\")\n",
                "with open('mmdetection_custom_part/mmdet/models/dense_heads/__init__.py', 'w') as f:\n",
                "    f.write(\"\"\"from .maskformer_head import MaskFormerHead\n",
                "from .mask2former_head import Mask2FormerHead\n",
                "__all__ = ['MaskFormerHead', 'Mask2FormerHead']\n",
                "\"\"\")\n",
                "with open('mmdetection_custom_part/mmdet/models/detectors/__init__.py', 'w') as f:\n",
                "    f.write(\"\"\"from .two_stage import TwoStageDetector\n",
                "from .faster_rcnn import FasterRCNN\n",
                "from .faster_rcnn_noise_inv import FasterRCNNNoiseInv\n",
                "from .mask_rcnn import MaskRCNN, MaskRCNNNoiseInv as MaskRCNNNoiseInvDet\n",
                "from .maskformer import MaskFormer\n",
                "from .mask2former import Mask2Former\n",
                "__all__ = ['TwoStageDetector', 'FasterRCNN', 'FasterRCNNNoiseInv',\n",
                "           'MaskRCNN', 'MaskRCNNNoiseInvDet', 'MaskFormer', 'Mask2Former']\n",
                "\"\"\")\n",
                "models_init = 'mmdetection_custom_part/mmdet/models/__init__.py'\n",
                "if os.path.exists(models_init):\n",
                "    with open(models_init, 'r') as f:\n",
                "        content = f.read()\n",
                "    content = content.replace('from .necks import *', '# from .necks import *')\n",
                "    with open(models_init, 'w') as f:\n",
                "        f.write(content)\n",
                "print('[Fix 6] __init__.py files rewritten.')\n",
                "print('\\nâœ… All fixes applied!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Verify Custom Modules"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "sys.path.insert(0, REPO_DIR)\n",
                "sys.path.insert(0, os.path.join(REPO_DIR, 'mmdetection'))\n",
                "\n",
                "try:\n",
                "    import mmdetection_custom_part.mmdet.models.detectors\n",
                "    import mmdetection_custom_part.mmdet.models.backbones\n",
                "    import mmdetection_custom_part.mmdet.models.dense_heads\n",
                "    import mmdetection_custom_part.mmdet.models.roi_heads\n",
                "    import mmdetection_custom_part.mmdet.models.plugins\n",
                "    import mmdetection_custom_part.mmdet.models.seg_heads\n",
                "    import mmdetection_custom_part.mmdet.models.losses\n",
                "    print('âœ… All custom modules imported successfully!')\n",
                "except Exception as e:\n",
                "    import traceback\n",
                "    traceback.print_exc()\n",
                "    raise RuntimeError(f'Custom module import failed: {e}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Setup Datasets"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(REPO_DIR)\n",
                "os.makedirs('data/coco/annotations', exist_ok=True)\n",
                "os.makedirs('data/coco/train2017', exist_ok=True)\n",
                "os.makedirs('data/eallis/annotations', exist_ok=True)\n",
                "os.makedirs('data/eallis/images', exist_ok=True)\n",
                "print('Data directories created.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7a. Download COCO"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DOWNLOAD_COCO = True  # Set False if using Drive symlinks\n",
                "\n",
                "if DOWNLOAD_COCO:\n",
                "    os.chdir(os.path.join(REPO_DIR, 'data/coco'))\n",
                "    !wget -q http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
                "    !unzip -qo annotations_trainval2017.zip\n",
                "    !rm annotations_trainval2017.zip\n",
                "    !wget -q http://images.cocodataset.org/zips/train2017.zip\n",
                "    !unzip -qo train2017.zip\n",
                "    !rm train2017.zip\n",
                "    os.chdir(REPO_DIR)\n",
                "    print(f'COCO train images: {len(os.listdir(\"data/coco/train2017\"))}')\n",
                "else:\n",
                "    print('Skipping COCO download.')\n",
                "    # Uncomment to symlink from Drive:\n",
                "    # !ln -sf /content/drive/MyDrive/datasets/coco/train2017 data/coco/train2017\n",
                "    # !ln -sf /content/drive/MyDrive/datasets/coco/annotations data/coco/annotations"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7b. Create 10% COCO Subset (Optional)\n",
                "\n",
                "Set `USE_SUBSET = False` for full COCO training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json\n",
                "import random\n",
                "\n",
                "os.chdir(REPO_DIR)\n",
                "\n",
                "USE_SUBSET = True       # Set False to train on full COCO\n",
                "SUBSET_RATIO = 0.10     # 10% of COCO (~11.8K images)\n",
                "\n",
                "if USE_SUBSET:\n",
                "    ann_path = 'data/coco/annotations/instances_train2017.json'\n",
                "    subset_path = 'data/coco/annotations/instances_train2017_subset.json'\n",
                "\n",
                "    print(f'Loading full COCO annotations...')\n",
                "    with open(ann_path, 'r') as f:\n",
                "        coco_data = json.load(f)\n",
                "\n",
                "    all_images = coco_data['images']\n",
                "    num_subset = int(len(all_images) * SUBSET_RATIO)\n",
                "\n",
                "    random.seed(42)\n",
                "    subset_images = random.sample(all_images, num_subset)\n",
                "    subset_img_ids = set(img['id'] for img in subset_images)\n",
                "    subset_annotations = [ann for ann in coco_data['annotations'] if ann['image_id'] in subset_img_ids]\n",
                "\n",
                "    subset_data = {\n",
                "        'info': coco_data.get('info', {}),\n",
                "        'licenses': coco_data.get('licenses', []),\n",
                "        'images': subset_images,\n",
                "        'annotations': subset_annotations,\n",
                "        'categories': coco_data['categories']\n",
                "    }\n",
                "\n",
                "    with open(subset_path, 'w') as f:\n",
                "        json.dump(subset_data, f)\n",
                "\n",
                "    print(f'âœ… Created {SUBSET_RATIO*100:.0f}% subset: {len(subset_images):,} images, {len(subset_annotations):,} annotations')\n",
                "else:\n",
                "    print('Using full COCO dataset.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7c. Upload EALLIS Dataset"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "EALLIS_DRIVE_PATH = '/content/drive/MyDrive/datasets/eallis'\n",
                "\n",
                "if os.path.exists(EALLIS_DRIVE_PATH):\n",
                "    if os.path.exists(os.path.join(EALLIS_DRIVE_PATH, 'annotations')):\n",
                "        !cp -r {EALLIS_DRIVE_PATH}/annotations/* {REPO_DIR}/data/eallis/annotations/\n",
                "    if os.path.exists(os.path.join(EALLIS_DRIVE_PATH, 'images')):\n",
                "        !cp -r {EALLIS_DRIVE_PATH}/images/* {REPO_DIR}/data/eallis/images/\n",
                "    elif os.path.exists(os.path.join(EALLIS_DRIVE_PATH, 'JPEGImages')):\n",
                "        !cp -r {EALLIS_DRIVE_PATH}/JPEGImages/* {REPO_DIR}/data/eallis/images/\n",
                "    print(f'EALLIS annotations: {os.listdir(REPO_DIR + \"/data/eallis/annotations\")}')\n",
                "    print(f'EALLIS images: {len(os.listdir(REPO_DIR + \"/data/eallis/images\"))} files')\n",
                "else:\n",
                "    print(f'EALLIS not found at {EALLIS_DRIVE_PATH}. Update the path above.')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(REPO_DIR)\n",
                "jpeg_link = 'data/eallis/JPEGImages'\n",
                "if not os.path.exists(jpeg_link):\n",
                "    os.symlink(os.path.abspath('data/eallis/images'), jpeg_link)\n",
                "    print(f'Created symlink: {jpeg_link} -> data/eallis/images')\n",
                "print(f'EALLIS images accessible: {os.path.exists(\"data/eallis/JPEGImages\")}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Prepare Training Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(REPO_DIR)\n",
                "\n",
                "CONFIG_FILE = 'Configs/mask_rcnn_r50_fpn_caffe_AWD_SCB_DSL_SynCOCO2EALLIS.py'\n",
                "\n",
                "with open(CONFIG_FILE, 'r') as f:\n",
                "    config_content = f.read()\n",
                "\n",
                "config_content = config_content.replace('BATCHSIZE = 8', 'BATCHSIZE = 4')\n",
                "\n",
                "config_content = re.sub(\n",
                "    r\"load_from\\s*=\\s*'[^']*'\",\n",
                "    \"load_from = None\",\n",
                "    config_content)\n",
                "\n",
                "if USE_SUBSET:\n",
                "    config_content = config_content.replace(\n",
                "        \"ann_file='data/coco/annotations/instances_train2017.json'\",\n",
                "        \"ann_file='data/coco/annotations/instances_train2017_subset.json'\")\n",
                "\n",
                "DRIVE_SAVE_DIR = '/content/drive/MyDrive/EALLIS_checkpoints'\n",
                "\n",
                "TRAIN_CONFIG = 'Configs/train_colab.py'\n",
                "with open(TRAIN_CONFIG, 'w') as f:\n",
                "    f.write(config_content)\n",
                "\n",
                "subset_label = f'{SUBSET_RATIO*100:.0f}% subset' if USE_SUBSET else 'full'\n",
                "print(f'Config saved: {TRAIN_CONFIG}')\n",
                "print(f'  Train: COCO ({subset_label}) â†’ SynCOCO | Val/Test: EALLIS | Batch: 4 | Epochs: 12')\n",
                "print(f'  Auto-save to: {DRIVE_SAVE_DIR}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Start Training ðŸš€\n",
                "\n",
                "Checkpoints are **automatically saved to Google Drive** after every epoch."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(REPO_DIR)\n",
                "\n",
                "import sys\n",
                "sys.path.insert(0, REPO_DIR)\n",
                "sys.path.insert(0, os.path.join(REPO_DIR, 'mmdetection'))\n",
                "\n",
                "import mmdetection_custom_part.mmdet.models.detectors\n",
                "import mmdetection_custom_part.mmdet.models.backbones\n",
                "import mmdetection_custom_part.mmdet.models.dense_heads\n",
                "import mmdetection_custom_part.mmdet.models.roi_heads\n",
                "import mmdetection_custom_part.mmdet.models.plugins\n",
                "import mmdetection_custom_part.mmdet.models.seg_heads\n",
                "import mmdetection_custom_part.mmdet.models.losses\n",
                "\n",
                "import shutil\n",
                "import torch\n",
                "from mmcv import Config\n",
                "from mmcv.runner import HOOKS, Hook\n",
                "from mmdet.datasets import build_dataset\n",
                "from mmdet.models import build_detector\n",
                "from mmdet.apis import train_detector\n",
                "\n",
                "@HOOKS.register_module(force=True)\n",
                "class DriveBackupHook(Hook):\n",
                "    \"\"\"Copies checkpoint files to Google Drive after each epoch.\"\"\"\n",
                "    def __init__(self, drive_dir):\n",
                "        self.drive_dir = drive_dir\n",
                "        os.makedirs(drive_dir, exist_ok=True)\n",
                "\n",
                "    def after_train_epoch(self, runner):\n",
                "        epoch = runner.epoch + 1\n",
                "        ckpt_name = f'epoch_{epoch}.pth'\n",
                "        src = os.path.join(runner.work_dir, ckpt_name)\n",
                "        if os.path.exists(src):\n",
                "            dst = os.path.join(self.drive_dir, ckpt_name)\n",
                "            shutil.copy2(src, dst)\n",
                "            runner.logger.info(f'ðŸ’¾ Saved {ckpt_name} to Google Drive')\n",
                "        for log_file in glob.glob(os.path.join(runner.work_dir, '*.log')):\n",
                "            shutil.copy2(log_file, os.path.join(self.drive_dir, os.path.basename(log_file)))\n",
                "\n",
                "cfg = Config.fromfile('Configs/train_colab.py')\n",
                "\n",
                "cfg.custom_hooks = cfg.get('custom_hooks', [])\n",
                "cfg.custom_hooks.append(dict(type='DriveBackupHook', drive_dir=DRIVE_SAVE_DIR, priority='LOW'))\n",
                "\n",
                "print('Building datasets...')\n",
                "datasets = [build_dataset(cfg.data.train)]\n",
                "print(f'Train dataset: {len(datasets[0])} images')\n",
                "\n",
                "print('Building model...')\n",
                "model = build_detector(cfg.model)\n",
                "model.init_weights()\n",
                "model.CLASSES = datasets[0].CLASSES\n",
                "\n",
                "print(f'Model: {cfg.model.type} | Backbone: {cfg.model.backbone.type}')\n",
                "print(f'Training for {cfg.runner.max_epochs} epochs | Auto-saving to: {DRIVE_SAVE_DIR}')\n",
                "\n",
                "train_detector(model, datasets, cfg, distributed=False, validate=True, meta=dict())\n",
                "\n",
                "print('\\nâœ… Training complete!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Evaluate on EALLIS Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(REPO_DIR)\n",
                "\n",
                "from mmcv.parallel import MMDataParallel\n",
                "from mmcv.runner import load_checkpoint\n",
                "from mmdet.datasets import build_dataloader\n",
                "from mmdet.apis import single_gpu_test\n",
                "\n",
                "import glob\n",
                "ckpt_files = sorted(glob.glob(os.path.join(cfg.work_dir, 'epoch_*.pth')))\n",
                "latest_ckpt = ckpt_files[-1] if ckpt_files else os.path.join(cfg.work_dir, 'latest.pth')\n",
                "print(f'Using checkpoint: {latest_ckpt}')\n",
                "\n",
                "test_dataset = build_dataset(cfg.data.test)\n",
                "test_loader = build_dataloader(test_dataset, samples_per_gpu=1, workers_per_gpu=2, dist=False, shuffle=False)\n",
                "\n",
                "cfg.model.pretrained = None\n",
                "cfg.model.backbone.init_cfg = None\n",
                "eval_model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
                "checkpoint = load_checkpoint(eval_model, latest_ckpt, map_location='cpu')\n",
                "eval_model.CLASSES = test_dataset.CLASSES\n",
                "eval_model = MMDataParallel(eval_model, device_ids=[0])\n",
                "eval_model.eval()\n",
                "\n",
                "print(f'Running inference on {len(test_dataset)} images...')\n",
                "results = single_gpu_test(eval_model, test_loader, show=False)\n",
                "\n",
                "eval_results = test_dataset.evaluate(results, metric=['bbox', 'segm'])\n",
                "print('\\n' + '='*60)\n",
                "print('EVALUATION RESULTS')\n",
                "print('='*60)\n",
                "for key, val in eval_results.items():\n",
                "    print(f'  {key}: {val:.4f}' if isinstance(val, float) else f'  {key}: {val}')\n",
                "print('='*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## Tips\n",
                "\n",
                "- **OOM errors**: Reduce `BATCHSIZE` in cell 8 (try 2 or 1)\n",
                "- **Faster training**: Use A100 via `Runtime > Change runtime type`\n",
                "- **Full COCO**: Set `USE_SUBSET = False` in cell 7b\n",
                "- **Resume training**: Set `resume_from = 'work_dir/epoch_X.pth'` in config\n",
                "- **Fine-tune from pretrained**: Set `load_from = 'Checkpoints/Checkpoint1.pth'`\n",
                "- Checkpoints are auto-saved to Google Drive after every epoch â€” no data loss on Colab timeout!"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}