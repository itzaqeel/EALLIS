{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# üåô EALLIS: Enhanced Adaptive Low-Light Instance Segmentation ‚Äî Kaggle Training\n",
                "\n",
                "**Model**: Mask R-CNN + AWD + SCB + DSL (`MaskRCNNNoiseInv` with `ResNetAdaDSmoothPrior`)  \n",
                "**Training**: COCO train2017 (from Kaggle dataset) with synthetic noise (SynCOCO)  \n",
                "**Evaluation**: EALLIS test set  \n",
                "\n",
                "---\n",
                "‚ö†Ô∏è **Requires**: GPU accelerator. Settings ‚Üí Accelerator ‚Üí GPU T4 x2 or P100."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Check"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "execution": {
                    "iopub.status.busy": "",
                    "iopub.execute_input": "",
                    "iopub.status.idle": "",
                    "shell.execute_reply": ""
                }
            },
            "outputs": [],
            "source": [
                "import torch, sys, os\n",
                "print(f'PyTorch: {torch.__version__}')\n",
                "print(f'CUDA:    {torch.version.cuda}')\n",
                "print(f'Python:  {sys.version}')\n",
                "print(f'GPU:     {torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"NONE ‚Äî enable GPU!\"}')\n",
                "\n",
                "# Kaggle paths\n",
                "KAGGLE_INPUT  = '/kaggle/input'\n",
                "KAGGLE_OUTPUT = '/kaggle/working'\n",
                "REPO_DIR      = os.path.join(KAGGLE_OUTPUT, 'EALLIS')\n",
                "SAVE_DIR      = os.path.join(KAGGLE_OUTPUT, 'outputs')   # wheels + checkpoints saved here\n",
                "os.makedirs(SAVE_DIR, exist_ok=True)\n",
                "\n",
                "# COCO dataset from Kaggle input\n",
                "COCO_ROOT = os.path.join(KAGGLE_INPUT, 'coco-2017-dataset', 'coco2017')\n",
                "print(f'\\nCOCO root: {COCO_ROOT}')\n",
                "print(f'  exists: {os.path.exists(COCO_ROOT)}')\n",
                "if os.path.exists(COCO_ROOT):\n",
                "    print(f'  contents: {os.listdir(COCO_ROOT)}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Install Dependencies\n",
                "\n",
                "Builds mmcv-full from source and caches the wheel in the output directory."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os, glob, sys\n",
                "import torch\n",
                "\n",
                "torch_ver = torch.__version__.split('+')[0]\n",
                "cuda_ver = torch.version.cuda.replace('.', '')\n",
                "\n",
                "# Check if mmcv is already installed\n",
                "try:\n",
                "    import mmcv\n",
                "    print(f'‚úÖ mmcv-full {mmcv.__version__} already installed!')\n",
                "except ImportError:\n",
                "    # Check for a previously saved wheel in output\n",
                "    cached_wheels = glob.glob(os.path.join(SAVE_DIR, 'mmcv_full-1.7.2*.whl'))\n",
                "    \n",
                "    if cached_wheels:\n",
                "        print(f'üì¶ Found cached wheel, installing...')\n",
                "        os.system(f'pip install -q {cached_wheels[0]}')\n",
                "    else:\n",
                "        print(f'‚è≥ Building mmcv-full from source (~15-25 min)...')\n",
                "        print(f'   PyTorch {torch_ver}, CUDA {cuda_ver}')\n",
                "        \n",
                "        # Build wheel\n",
                "        os.system(f'pip wheel mmcv-full==1.7.2 -w /tmp/mmcv_wheels '\n",
                "                  f'-f https://download.openmmlab.com/mmcv/dist/cu{cuda_ver}/torch{torch_ver}/index.html '\n",
                "                  f'2>&1 | tail -5')\n",
                "        \n",
                "        built = glob.glob('/tmp/mmcv_wheels/mmcv_full-1.7.2*.whl')\n",
                "        if built:\n",
                "            os.system(f'pip install -q {built[0]}')\n",
                "            # Save wheel to output for download / reuse\n",
                "            import shutil\n",
                "            shutil.copy2(built[0], SAVE_DIR)\n",
                "            print(f'üíæ Wheel saved to {SAVE_DIR}')\n",
                "        else:\n",
                "            os.system(f'pip install mmcv-full==1.7.2 '\n",
                "                      f'-f https://download.openmmlab.com/mmcv/dist/cu{cuda_ver}/torch{torch_ver}/index.html')\n",
                "    \n",
                "    import mmcv\n",
                "    print(f'‚úÖ mmcv-full {mmcv.__version__} installed!')\n",
                "\n",
                "!pip install -q pycocotools scikit-learn terminaltables pretrainedmodels\n",
                "print('‚úÖ All dependencies ready!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Clone EALLIS Repository"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(KAGGLE_OUTPUT)\n",
                "\n",
                "if not os.path.exists(REPO_DIR):\n",
                "    !git clone https://github.com/itzaqeel/EALLIS.git {REPO_DIR}\n",
                "else:\n",
                "    os.chdir(REPO_DIR)\n",
                "    !git pull\n",
                "\n",
                "os.chdir(REPO_DIR)\n",
                "print(f'Working directory: {os.getcwd()}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Install mmdetection"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(os.path.join(REPO_DIR, 'mmdetection'))\n",
                "!pip install -q -e . --no-deps\n",
                "os.chdir(REPO_DIR)\n",
                "\n",
                "sys.path.insert(0, os.path.join(REPO_DIR, 'mmdetection'))\n",
                "\n",
                "import mmdet\n",
                "print(f'mmdet version: {mmdet.__version__}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Apply All Compatibility Fixes\n",
                "\n",
                "Python 3.10/3.12, NumPy 2.x, mmcv 1.7.2, custom module imports."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import glob, re\n",
                "os.chdir(REPO_DIR)\n",
                "\n",
                "# --- Fix 1: Bump mmcv version cap ---\n",
                "init_file = 'mmdetection/mmdet/__init__.py'\n",
                "with open(init_file, 'r') as f:\n",
                "    content = f.read()\n",
                "content = re.sub(r\"mmcv_maximum_version\\s*=\\s*'[^']*'\", \"mmcv_maximum_version = '3.0.0'\", content)\n",
                "with open(init_file, 'w') as f:\n",
                "    f.write(content)\n",
                "print('[Fix 1] mmcv version cap updated.')\n",
                "\n",
                "# --- Fix 2: Deprecated/removed imports ---\n",
                "deprecated_imports = {\n",
                "    'import imp': '# import imp  # removed in Python 3.12',\n",
                "    'from os import pread': '# from os import pread',\n",
                "    'from tokenize import group': '# from tokenize import group',\n",
                "    'from numpy.core.fromnumeric import size': '# from numpy.core.fromnumeric import size',\n",
                "    'from numpy.core.numeric import outer': '# from numpy.core.numeric import outer',\n",
                "    'from numpy.lib.npyio import load': '# from numpy.lib.npyio import load',\n",
                "    'from numpy.lib.arraypad import pad': 'from numpy import pad',\n",
                "    'from numpy.lib.type_check import common_type': 'from numpy import common_type',\n",
                "    'from torch.functional import _index_tensor_with_indices_list': '# from torch.functional import _index_tensor_with_indices_list',\n",
                "    'from numpy.testing._private.utils import print_assert_equal': '# from numpy.testing._private.utils import print_assert_equal',\n",
                "}\n",
                "fix2_files = glob.glob('mmdetection/**/*.py', recursive=True) + \\\n",
                "             glob.glob('mmdetection_custom_part/**/*.py', recursive=True) + \\\n",
                "             glob.glob('utils/**/*.py', recursive=True)\n",
                "fix2_count = 0\n",
                "for py_file in fix2_files:\n",
                "    with open(py_file, 'r') as f:\n",
                "        content = f.read()\n",
                "    new_content = content\n",
                "    for old_imp, new_imp in deprecated_imports.items():\n",
                "        new_content = new_content.replace(old_imp, new_imp)\n",
                "    if new_content != content:\n",
                "        with open(py_file, 'w') as f:\n",
                "            f.write(new_content)\n",
                "        fix2_count += 1\n",
                "print(f'[Fix 2] Fixed deprecated imports in {fix2_count} files.')\n",
                "\n",
                "# --- Fix 3: Relative imports in custom_part ---\n",
                "import_fixes = [\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/resnet.py', 'from ..utils', 'from mmdet.models.utils'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/resnet.py', 'from .cbam', 'from mmdet.models.backbones.cbam'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/resnext.py', 'from ..utils', 'from mmdet.models.utils'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/swin.py', 'from ...utils', 'from mmdet.utils'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/swin.py', 'from ..utils.ckpt_convert', 'from mmdet.models.utils.ckpt_convert'),\n",
                "    ('mmdetection_custom_part/mmdet/models/backbones/swin.py', 'from ..utils.transformer', 'from mmdet.models.utils.transformer'),\n",
                "    ('mmdetection_custom_part/mmdet/models/dense_heads/maskformer_head.py', 'from .anchor_free_head', 'from mmdet.models.dense_heads.anchor_free_head'),\n",
                "    ('mmdetection_custom_part/mmdet/models/dense_heads/mask2former_head.py', 'from .anchor_free_head', 'from mmdet.models.dense_heads.anchor_free_head'),\n",
                "    ('mmdetection_custom_part/mmdet/models/detectors/two_stage.py', 'from .base', 'from mmdet.models.detectors.base'),\n",
                "    ('mmdetection_custom_part/mmdet/models/detectors/maskformer.py', 'from .single_stage', 'from mmdet.models.detectors.single_stage'),\n",
                "    ('mmdetection_custom_part/mmdet/models/seg_heads/base_semantic_head.py', 'from ..utils import interpolate_as', 'from mmdet.models.utils import interpolate_as'),\n",
                "    ('mmdetection_custom_part/mmdet/models/seg_heads/panoptic_fpn_head.py', 'from ..utils import ConvUpsample', 'from mmdet.models.utils import ConvUpsample'),\n",
                "]\n",
                "for filepath, old_imp, new_imp in import_fixes:\n",
                "    if os.path.exists(filepath):\n",
                "        with open(filepath, 'r') as f:\n",
                "            content = f.read()\n",
                "        if old_imp in content:\n",
                "            content = content.replace(old_imp, new_imp)\n",
                "            with open(filepath, 'w') as f:\n",
                "                f.write(content)\n",
                "print('[Fix 3] Relative imports fixed.')\n",
                "\n",
                "# --- Fix 4: Detector auxiliary imports ---\n",
                "for det_file in [\n",
                "    'mmdetection_custom_part/mmdet/models/detectors/mask_rcnn.py',\n",
                "    'mmdetection_custom_part/mmdet/models/detectors/faster_rcnn_noise_inv.py'\n",
                "]:\n",
                "    if os.path.exists(det_file):\n",
                "        with open(det_file, 'r') as f:\n",
                "            content = f.read()\n",
                "        content = content.replace('from ..backbones.aux_modules', 'from mmdet.models.backbones.aux_modules')\n",
                "        content = content.replace('from ..backbones.multiscale_discriminator', 'from mmdet.models.backbones.multiscale_discriminator')\n",
                "        content = content.replace('from ..backbones.lsid', 'from mmdet.models.backbones.lsid')\n",
                "        with open(det_file, 'w') as f:\n",
                "            f.write(content)\n",
                "print('[Fix 4] Detector auxiliary imports fixed.')\n",
                "\n",
                "# --- Fix 5: Force register_module() ---\n",
                "count = 0\n",
                "for py_file in glob.glob('mmdetection_custom_part/**/*.py', recursive=True):\n",
                "    with open(py_file, 'r') as f:\n",
                "        content = f.read()\n",
                "    if '.register_module()' in content:\n",
                "        with open(py_file, 'w') as f:\n",
                "            f.write(content.replace('.register_module()', '.register_module(force=True)'))\n",
                "        count += 1\n",
                "print(f'[Fix 5] Forced registration in {count} files.')\n",
                "\n",
                "# --- Fix 6: Rewrite __init__.py files ---\n",
                "with open('mmdetection_custom_part/mmdet/models/backbones/__init__.py', 'w') as f:\n",
                "    f.write(\"\"\"from .resnet import ResNet, ResNetV1d, ResNetAdaD, ResNetAdaDSmoothPrior\n",
                "from .resnext import ResNeXt\n",
                "from .swin import SwinTransformer, SwinTransformerAdaD\n",
                "from .convnext import ConvNeXt, ConvNeXtAdaD\n",
                "__all__ = ['ResNet', 'ResNetV1d', 'ResNetAdaD', 'ResNetAdaDSmoothPrior',\n",
                "           'ResNeXt', 'SwinTransformer', 'SwinTransformerAdaD',\n",
                "           'ConvNeXt', 'ConvNeXtAdaD']\n",
                "\"\"\")\n",
                "with open('mmdetection_custom_part/mmdet/models/dense_heads/__init__.py', 'w') as f:\n",
                "    f.write(\"\"\"from .maskformer_head import MaskFormerHead\n",
                "from .mask2former_head import Mask2FormerHead\n",
                "__all__ = ['MaskFormerHead', 'Mask2FormerHead']\n",
                "\"\"\")\n",
                "with open('mmdetection_custom_part/mmdet/models/detectors/__init__.py', 'w') as f:\n",
                "    f.write(\"\"\"from .two_stage import TwoStageDetector\n",
                "from .faster_rcnn import FasterRCNN\n",
                "from .faster_rcnn_noise_inv import FasterRCNNNoiseInv\n",
                "from .mask_rcnn import MaskRCNN, MaskRCNNNoiseInv as MaskRCNNNoiseInvDet\n",
                "from .maskformer import MaskFormer\n",
                "from .mask2former import Mask2Former\n",
                "__all__ = ['TwoStageDetector', 'FasterRCNN', 'FasterRCNNNoiseInv',\n",
                "           'MaskRCNN', 'MaskRCNNNoiseInvDet', 'MaskFormer', 'Mask2Former']\n",
                "\"\"\")\n",
                "models_init = 'mmdetection_custom_part/mmdet/models/__init__.py'\n",
                "if os.path.exists(models_init):\n",
                "    with open(models_init, 'r') as f:\n",
                "        content = f.read()\n",
                "    content = content.replace('from .necks import *', '# from .necks import *')\n",
                "    with open(models_init, 'w') as f:\n",
                "        f.write(content)\n",
                "print('[Fix 6] __init__.py files rewritten.')\n",
                "\n",
                "# --- Fix 7: NoiseModel camera_params path ---\n",
                "noise_file = 'mmdetection/mmdet/datasets/pipelines/noisemodel/dark_noising.py'\n",
                "with open(noise_file, 'r') as f:\n",
                "    content = f.read()\n",
                "if '~/code/mmdetection' in content:\n",
                "    if 'import os\\n' not in content:\n",
                "        content = content.replace('import os.path as osp', 'import os\\nimport os.path as osp')\n",
                "    content = content.replace(\n",
                "        \"\"\"        if param_dir is None:\n",
                "            try:\n",
                "                self.param_dir = '~/code/mmdetection/mmdet/datasets/pipelines/noisemodel/camera_params'\n",
                "            except:\n",
                "                print('please specify the location of camera parameters, e.g., ~/code/mmdetection/mmdet/datasets/pipelines/noisemodel/camera_params')\n",
                "                raise Exception\"\"\",\n",
                "        \"\"\"        if param_dir is None:\n",
                "            self.param_dir = os.path.join(os.path.dirname(__file__), 'camera_params')\"\"\")\n",
                "    with open(noise_file, 'w') as f:\n",
                "        f.write(content)\n",
                "    print('[Fix 7] NoiseModel camera_params path fixed.')\n",
                "else:\n",
                "    print('[Fix 7] NoiseModel path already fixed.')\n",
                "\n",
                "print('\\n‚úÖ All fixes applied!')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Verify Custom Modules"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "sys.path.insert(0, REPO_DIR)\n",
                "sys.path.insert(0, os.path.join(REPO_DIR, 'mmdetection'))\n",
                "\n",
                "try:\n",
                "    import mmdetection_custom_part.mmdet.models.detectors\n",
                "    import mmdetection_custom_part.mmdet.models.backbones\n",
                "    import mmdetection_custom_part.mmdet.models.dense_heads\n",
                "    import mmdetection_custom_part.mmdet.models.roi_heads\n",
                "    import mmdetection_custom_part.mmdet.models.plugins\n",
                "    import mmdetection_custom_part.mmdet.models.seg_heads\n",
                "    import mmdetection_custom_part.mmdet.models.losses\n",
                "    print('‚úÖ All custom modules imported successfully!')\n",
                "except Exception as e:\n",
                "    import traceback\n",
                "    traceback.print_exc()\n",
                "    raise RuntimeError(f'Custom module import failed: {e}')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Setup Datasets\n",
                "\n",
                "Symlinks Kaggle's pre-loaded COCO dataset into the expected `data/coco/` structure. **No copying needed ‚Äî instant.**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "os.chdir(REPO_DIR)\n",
                "\n",
                "# --- COCO: Symlink from Kaggle input (read-only, no copy needed) ---\n",
                "os.makedirs('data/coco', exist_ok=True)\n",
                "\n",
                "for folder in ['annotations', 'train2017', 'val2017', 'test2017']:\n",
                "    src = os.path.join(COCO_ROOT, folder)\n",
                "    dst = os.path.join(REPO_DIR, 'data', 'coco', folder)\n",
                "    if os.path.islink(dst):\n",
                "        os.unlink(dst)\n",
                "    elif os.path.isdir(dst):\n",
                "        shutil.rmtree(dst)\n",
                "    if os.path.exists(src):\n",
                "        os.symlink(src, dst)\n",
                "        print(f'  üîó {folder}/ ‚Üí {src}')\n",
                "    else:\n",
                "        print(f'  ‚ö†Ô∏è  {src} not found')\n",
                "\n",
                "train_count = len(os.listdir('data/coco/train2017'))\n",
                "ann_files = os.listdir('data/coco/annotations')\n",
                "print(f'\\n‚úÖ COCO ready: {train_count:,} train images, {len(ann_files)} annotation files')\n",
                "\n",
                "# --- EALLIS dataset ---\n",
                "os.makedirs('data/eallis/annotations', exist_ok=True)\n",
                "os.makedirs('data/eallis/images', exist_ok=True)\n",
                "\n",
                "# Check if EALLIS is available as a Kaggle dataset input\n",
                "eallis_kaggle = os.path.join(KAGGLE_INPUT, 'eallis')\n",
                "if os.path.exists(eallis_kaggle):\n",
                "    for sub in ['annotations', 'images', 'JPEGImages']:\n",
                "        src = os.path.join(eallis_kaggle, sub)\n",
                "        if os.path.exists(src):\n",
                "            target = 'data/eallis/images' if sub == 'JPEGImages' else f'data/eallis/{sub}'\n",
                "            !cp -rn {src}/* {target}/\n",
                "    print(f'EALLIS images: {len(os.listdir(\"data/eallis/images\"))} files')\n",
                "else:\n",
                "    print(f'‚ÑπÔ∏è  EALLIS dataset not found at {eallis_kaggle}.')\n",
                "    print(f'   Add it as a Kaggle dataset input, or upload to data/eallis/ manually.')\n",
                "\n",
                "# JPEGImages symlink for EALLIS\n",
                "jpeg_link = 'data/eallis/JPEGImages'\n",
                "if not os.path.exists(jpeg_link):\n",
                "    os.symlink(os.path.abspath('data/eallis/images'), jpeg_link)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 7b. Create 10% COCO Subset (Optional)\n",
                "\n",
                "Set `USE_SUBSET = False` for full COCO training."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import json, random\n",
                "os.chdir(REPO_DIR)\n",
                "\n",
                "USE_SUBSET = True       # Set False for full COCO\n",
                "SUBSET_RATIO = 0.10     # 10%\n",
                "\n",
                "if USE_SUBSET:\n",
                "    ann_path = 'data/coco/annotations/instances_train2017.json'\n",
                "    # Write subset to writable location (Kaggle input is read-only)\n",
                "    os.makedirs('data/coco_subset', exist_ok=True)\n",
                "    subset_path = 'data/coco_subset/instances_train2017_subset.json'\n",
                "\n",
                "    print('Loading full COCO annotations...')\n",
                "    with open(ann_path, 'r') as f:\n",
                "        coco_data = json.load(f)\n",
                "\n",
                "    all_images = coco_data['images']\n",
                "    num_subset = int(len(all_images) * SUBSET_RATIO)\n",
                "    random.seed(42)\n",
                "    subset_images = random.sample(all_images, num_subset)\n",
                "    subset_img_ids = set(img['id'] for img in subset_images)\n",
                "    subset_annotations = [a for a in coco_data['annotations'] if a['image_id'] in subset_img_ids]\n",
                "\n",
                "    subset_data = {\n",
                "        'info': coco_data.get('info', {}),\n",
                "        'licenses': coco_data.get('licenses', []),\n",
                "        'images': subset_images,\n",
                "        'annotations': subset_annotations,\n",
                "        'categories': coco_data['categories']\n",
                "    }\n",
                "    with open(subset_path, 'w') as f:\n",
                "        json.dump(subset_data, f)\n",
                "\n",
                "    print(f'‚úÖ {SUBSET_RATIO*100:.0f}% subset: {len(subset_images):,} images, {len(subset_annotations):,} annotations')\n",
                "else:\n",
                "    print('Using full COCO dataset.')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Prepare Training Config"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(REPO_DIR)\n",
                "\n",
                "CONFIG_FILE = 'Configs/mask_rcnn_r50_fpn_caffe_AWD_SCB_DSL_SynCOCO2EALLIS.py'\n",
                "\n",
                "with open(CONFIG_FILE, 'r') as f:\n",
                "    config_content = f.read()\n",
                "\n",
                "# Batch size for Kaggle GPU (T4 = 16GB, P100 = 16GB)\n",
                "config_content = config_content.replace('BATCHSIZE = 8', 'BATCHSIZE = 4')\n",
                "\n",
                "# No pretrained checkpoint on first run\n",
                "config_content = re.sub(\n",
                "    r\"load_from\\s*=\\s*'[^']*'\",\n",
                "    \"load_from = None\",\n",
                "    config_content)\n",
                "\n",
                "# Use subset annotation file (writable location)\n",
                "if USE_SUBSET:\n",
                "    config_content = config_content.replace(\n",
                "        \"ann_file='data/coco/annotations/instances_train2017.json'\",\n",
                "        \"ann_file='data/coco_subset/instances_train2017_subset.json'\")\n",
                "\n",
                "# Set work_dir to Kaggle output\n",
                "config_content = config_content.replace(\n",
                "    \"work_dir = './work_dir'\",\n",
                "    f\"work_dir = '{SAVE_DIR}'\")\n",
                "\n",
                "TRAIN_CONFIG = 'Configs/train_kaggle.py'\n",
                "with open(TRAIN_CONFIG, 'w') as f:\n",
                "    f.write(config_content)\n",
                "\n",
                "subset_label = f'{SUBSET_RATIO*100:.0f}% subset' if USE_SUBSET else 'full'\n",
                "print(f'Config saved: {TRAIN_CONFIG}')\n",
                "print(f'  Train: COCO ({subset_label}) | Batch: 4 | Epochs: 12')\n",
                "print(f'  Checkpoints ‚Üí {SAVE_DIR}  (downloadable from Kaggle Output)')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Start Training üöÄ\n",
                "\n",
                "Only the **best checkpoint** (by bbox mAP) is kept. All outputs are saved to `/kaggle/working/outputs/` ‚Äî downloadable from the **Output** tab after the run."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(REPO_DIR)\n",
                "\n",
                "sys.path.insert(0, REPO_DIR)\n",
                "sys.path.insert(0, os.path.join(REPO_DIR, 'mmdetection'))\n",
                "\n",
                "import mmdetection_custom_part.mmdet.models.detectors\n",
                "import mmdetection_custom_part.mmdet.models.backbones\n",
                "import mmdetection_custom_part.mmdet.models.dense_heads\n",
                "import mmdetection_custom_part.mmdet.models.roi_heads\n",
                "import mmdetection_custom_part.mmdet.models.plugins\n",
                "import mmdetection_custom_part.mmdet.models.seg_heads\n",
                "import mmdetection_custom_part.mmdet.models.losses\n",
                "\n",
                "import shutil, torch, glob\n",
                "from mmcv import Config\n",
                "from mmcv.runner import HOOKS, Hook\n",
                "from mmdet.datasets import build_dataset\n",
                "from mmdet.models import build_detector\n",
                "from mmdet.apis import train_detector\n",
                "\n",
                "cfg = Config.fromfile('Configs/train_kaggle.py')\n",
                "\n",
                "print('Building datasets...')\n",
                "datasets = [build_dataset(cfg.data.train)]\n",
                "print(f'Train dataset: {len(datasets[0])} images')\n",
                "\n",
                "print('Building model...')\n",
                "model = build_detector(cfg.model)\n",
                "model.init_weights()\n",
                "model.CLASSES = datasets[0].CLASSES\n",
                "\n",
                "print(f'Model: {cfg.model.type} | Backbone: {cfg.model.backbone.type}')\n",
                "print(f'Training for {cfg.runner.max_epochs} epochs')\n",
                "print(f'Checkpoints saved to: {cfg.work_dir}')\n",
                "\n",
                "train_detector(model, datasets, cfg, distributed=False, validate=True, meta=dict())\n",
                "\n",
                "print('\\n‚úÖ Training complete!')\n",
                "print(f'\\nüìÅ Output files (downloadable from Kaggle Output tab):')\n",
                "for f in sorted(glob.glob(os.path.join(SAVE_DIR, '*'))):\n",
                "    size_mb = os.path.getsize(f) / 1024 / 1024\n",
                "    print(f'  {os.path.basename(f):40s} {size_mb:8.1f} MB')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Evaluate on EALLIS Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "os.chdir(REPO_DIR)\n",
                "\n",
                "from mmcv.parallel import MMDataParallel\n",
                "from mmcv.runner import load_checkpoint\n",
                "from mmdet.datasets import build_dataloader\n",
                "from mmdet.apis import single_gpu_test\n",
                "\n",
                "# Find best checkpoint\n",
                "best_ckpts = sorted(glob.glob(os.path.join(SAVE_DIR, 'best_*.pth')))\n",
                "if best_ckpts:\n",
                "    ckpt_path = best_ckpts[-1]\n",
                "else:\n",
                "    epoch_ckpts = sorted(glob.glob(os.path.join(SAVE_DIR, 'epoch_*.pth')))\n",
                "    ckpt_path = epoch_ckpts[-1] if epoch_ckpts else os.path.join(SAVE_DIR, 'latest.pth')\n",
                "print(f'Using checkpoint: {ckpt_path}')\n",
                "\n",
                "test_dataset = build_dataset(cfg.data.test)\n",
                "test_loader = build_dataloader(test_dataset, samples_per_gpu=1, workers_per_gpu=2, dist=False, shuffle=False)\n",
                "\n",
                "cfg.model.pretrained = None\n",
                "cfg.model.backbone.init_cfg = None\n",
                "eval_model = build_detector(cfg.model, test_cfg=cfg.get('test_cfg'))\n",
                "checkpoint = load_checkpoint(eval_model, ckpt_path, map_location='cpu')\n",
                "eval_model.CLASSES = test_dataset.CLASSES\n",
                "eval_model = MMDataParallel(eval_model, device_ids=[0])\n",
                "eval_model.eval()\n",
                "\n",
                "print(f'Running inference on {len(test_dataset)} images...')\n",
                "results = single_gpu_test(eval_model, test_loader, show=False)\n",
                "\n",
                "eval_results = test_dataset.evaluate(results, metric=['bbox', 'segm'])\n",
                "print('\\n' + '='*60)\n",
                "print('EVALUATION RESULTS')\n",
                "print('='*60)\n",
                "for key, val in eval_results.items():\n",
                "    print(f'  {key}: {val:.4f}' if isinstance(val, float) else f'  {key}: {val}')\n",
                "print('='*60)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Download Summary\n",
                "\n",
                "After training, go to **Kaggle ‚Üí Output tab** to download:\n",
                "- `best_model.pth` ‚Äî best checkpoint by bbox mAP\n",
                "- `mmcv_full-*.whl` ‚Äî compiled mmcv wheel (reusable)\n",
                "- Training logs\n",
                "\n",
                "---\n",
                "### Tips\n",
                "- **OOM**: Reduce batch size in Cell 8 (try 2)\n",
                "- **Full COCO**: Set `USE_SUBSET = False` in Cell 7b\n",
                "- **Faster GPU**: Use P100 or T4 x2 from Kaggle accelerator settings\n",
                "- **Resume**: Upload a checkpoint as Kaggle dataset input and set `resume_from`\n",
                "- Kaggle gives **30h/week GPU** ‚Äî enough for several training runs"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "kaggle": {
            "accelerator": "gpu",
            "dataSources": [
                {
                    "sourceId": 37968,
                    "sourceType": "datasetVersion",
                    "datasetSlug": "coco-2017-dataset",
                    "isSourceIdPinned": false
                }
            ],
            "isGpuEnabled": true,
            "isInternetEnabled": true,
            "language": "python",
            "sourceType": "notebook"
        },
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}